{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26088c96",
   "metadata": {},
   "source": [
    "This notebook can be used to get you started in your project. It contains functions to fetch data from wikipedia and parse through some of the source data. The data for politicians is already prefetched for you, and you can download the \"politicians.json\" through A+.\n",
    "\n",
    "For Wikipedia API see documentation here:\n",
    "https://pypi.org/project/Wikipedia-API/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e59646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi, time, json,requests,os\n",
    "#from tqdm import tqdm # you can import this for progress bar instead if you are not using notebooks\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def ensure_person_data():\n",
    "    \"\"\"Ensures the existence of the person-data.tsv file.\n",
    "    \n",
    "    For downloading the file 'person-data.tsv', please go to https://search.gesis.org/research_data/SDN-10.7802-1515\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the person-data.tsv file is not found in the current directory.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(\"person-data.tsv\"):\n",
    "        raise Exception(\"For downloading the file 'person-data.tsv', please go to https://search.gesis.org/research_data/SDN-10.7802-1515\")\n",
    "    \n",
    "def ensure_gender_data():\n",
    "    \"\"\"Ensures the existence of the gender data file and downloads it from a remote URL if it is not found.\n",
    "    \n",
    "    The file is downloaded from http://www.cs.cmu.edu/~ark/bio/data/wiki.genders.txt\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(\"wiki.genders.txt\"):\n",
    "        print(\"Downloading the gender data file...\")\n",
    "        open('wiki.genders.txt', 'wb').write(requests.get(\"http://www.cs.cmu.edu/~ark/bio/data/wiki.genders.txt\", allow_redirects=True).content)\n",
    "    \n",
    "    \n",
    "def filter_persons_by(occupation=None,birth_less=None,birth_more=None,nationality=None):\n",
    "    \"\"\"\n",
    "    Filters persons from the person-data.tsv file based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        occupation (str, optional): The occupation of the person. Defaults to None.\n",
    "        birth_less (int, optional): The upper bound of the birth year of the person. Defaults to None.\n",
    "        birth_more (int, optional): The lower bound of the birth year of the person. Defaults to None.\n",
    "        nationality (str, optional): The nationality of the person. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of persons that match the specified criteria. The keys are the person names and the values are \n",
    "        dictionaries containing the person's attributes.\n",
    "    \"\"\"\n",
    "    ensure_person_data()\n",
    "    pfile=open(\"person-data.tsv\",'r', encoding='utf-8')\n",
    "    titles=pfile.readline().strip().split(\"\\t\")\n",
    "    i=0\n",
    "    persons={}\n",
    "    for line in pfile:\n",
    "        person=dict(zip(titles,line.strip().split(\"\\t\")))\n",
    "        if person[\"birthDate\"]=='NA':\n",
    "            birthYear=None\n",
    "        else:\n",
    "            birthDate=person[\"birthDate\"].strip(\"[]\\t' \")\n",
    "            birthYear=int(birthDate.strip(\"-\").split(\"-\")[0])\n",
    "            if birthDate[0]==\"-\":\n",
    "                birthYear=-birthYear\n",
    "        \n",
    "        occupation_ok=occupation==None or occupation in person[\"occupation\"] \n",
    "        nationality_ok=nationality==None or nationality in person[\"nationality\"] \n",
    "        birth_less_ok=birth_less==None or birthYear!=None and birthYear<birth_less\n",
    "        birth_more_ok=birth_more==None or birthYear!=None and birthYear>birth_more\n",
    "               \n",
    "        if occupation_ok and nationality_ok and birth_less_ok and birth_more_ok:\n",
    "            name=person[\"WikiURL\"][len(\"http://en.wikipedia.org/wiki/\"):]\n",
    "            persons[name]=person\n",
    "    return persons\n",
    "\n",
    "def get_genderdata():\n",
    "    \"\"\"Reads a tab-separated file containing Wikipedia article information and returns a dictionary of gender data.\n",
    "\n",
    "    The function reads a file named \"wiki.genders.txt\" and extracts the gender data for each name in the file, using the first letter of the gender field. The gender data is then stored in a dictionary with the name as the key and the gender abbreviation as the value.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing gender data for each name in the file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the input file cannot be found or opened.\n",
    "\n",
    "    Example:\n",
    "        >>> gender_data = get_genderdata()\n",
    "        >>> gender_data['Albert_Einstein']\n",
    "        'M'\n",
    "    \"\"\"\n",
    "    ensure_gender_data()\n",
    "    genderdata={}\n",
    "    with open(\"wiki.genders.txt\", \"r\", encoding='utf-8') as inputfile:\n",
    "        inputfile.readline()\n",
    "        for line in inputfile:\n",
    "            wid,gender,name=line.strip().split(\"\\t\")\n",
    "            name=name.replace(\" \",\"_\")\n",
    "            genderdata[name]=gender[:1]\n",
    "    return genderdata\n",
    "\n",
    "def fill_in_genders(persons):\n",
    "    \"\"\"\n",
    "    Fills in the gender information of persons in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        persons (dict): A dictionary containing information about persons.\n",
    "\n",
    "    Returns:\n",
    "        None. The function modifies the input dictionary in place.\n",
    "\n",
    "    Examples:\n",
    "        >>> persons = {'Alice': {'age': 25}, 'Bob': {'age': 30}}\n",
    "        >>> fill_in_genders(persons)\n",
    "        >>> persons\n",
    "        {'Alice': {'age': 25, 'gender': 'F'}, 'Bob': {'age': 30, 'gender': 'M'}}\n",
    "\n",
    "    \"\"\"\n",
    "    genderdata=get_genderdata()\n",
    "    for person in list(persons.keys()):\n",
    "        if person in genderdata:\n",
    "            gender=genderdata[person]\n",
    "        else:\n",
    "            gender=\"NA\"\n",
    "        persons[person][\"gender\"]=gender\n",
    "        \n",
    "def fetch_links(people,batch_size=None,lang='en'):\n",
    "    \"\"\"Uses the Wikipedia API to fetch Wikipedia links between the given people.\n",
    "    \n",
    "    The links are filled into the people dictionary in place.\n",
    "    \n",
    "    Note that only links between the people are saved, and if you want to inspect other links\n",
    "    you should write your own fetching function.\n",
    "\n",
    "    Args:\n",
    "        people (dict): A dictionary containing names of people as keys and attributes as values.\n",
    "        batch_size (int, optional): The maximum number of people to fetch links for in a single batch. Defaults to None, which means there is no maximum.\n",
    "        lang (str, optional): The language in which to fetch Wikipedia links. Defaults to 'en'.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the links were not fetched for every person due to the batch size, False otherwise.\n",
    "    \"\"\"\n",
    "    wiki = wikipediaapi.Wikipedia(lang)\n",
    "    i=0\n",
    "    print('Fetching link data from Wikipedia')\n",
    "    pbar=tqdm(total=len(people))\n",
    "    for name,attributes in people.items():\n",
    "        pbar.update(1)\n",
    "        if \"links\" not in attributes:\n",
    "            page=wiki.page(name)\n",
    "            links=list(map(lambda x:x.replace(\" \",\"_\"),page.links.keys()))\n",
    "            plinks=list(filter(lambda x:x in people,links))\n",
    "            #print(name,plinks)\n",
    "            people[name][\"links\"]=plinks\n",
    "            i+=1\n",
    "            time.sleep(0.1)\n",
    "        if i==batch_size:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fetch_langs(people,batch_size=None,lang='en'):\n",
    "    \"\"\"Uses the Wikipedia API to fetch list of Wikipedia language editions where each person in the people \n",
    "    dictionary appears.\n",
    "    \n",
    "    The language editions are filled into the people dictionary in place.\n",
    "\n",
    "    Args:\n",
    "        people (dict): A dictionary containing names of people as keys and attributes as values.\n",
    "        batch_size (int, optional): The maximum number of people to fetch links for in a single batch. Defaults to None, which means there is no maximum.\n",
    "        lang (str, optional): The language in which to fetch Wikipedia links. Defaults to 'en'.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the language editions were not fetched for every person due to the batch size, False otherwise.\n",
    "    \"\"\"\n",
    "    wiki = wikipediaapi.Wikipedia(lang)\n",
    "    i=0\n",
    "    print('Fetching language editions data from Wikipedia')\n",
    "    pbar=tqdm(total=len(people))\n",
    "    for name,attributes in people.items():\n",
    "        pbar.update(1)\n",
    "        if \"langs\" not in attributes:\n",
    "            page=wiki.page(name)\n",
    "            langs=list(page.langlinks.keys())\n",
    "            #print(name,langs)\n",
    "            people[name][\"langs\"]=langs\n",
    "            i+=1\n",
    "            time.sleep(0.1)\n",
    "        if i==batch_size:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def fetch_summaries(people,batch_size=None,lang='en'):\n",
    "    \"\"\"Uses the Wikipedia API to fetch summary texts for each person in the people dictionary.\n",
    "    \n",
    "    The summary texts are filled into the people dictionary in place.\n",
    "\n",
    "    Args:\n",
    "        people (dict): A dictionary containing names of people as keys and attributes as values.\n",
    "        batch_size (int, optional): The maximum number of people to fetch links for in a single batch. Defaults to None, which means there is no maximum.\n",
    "        lang (str, optional): The language in which to fetch Wikipedia links. Defaults to 'en'.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the summaries were not fetched for every person due to the batch size, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    wiki = wikipediaapi.Wikipedia(lang)\n",
    "    i=0\n",
    "    print('Fetching summary text data from Wikipedia')\n",
    "    pbar=tqdm(total=len(people))\n",
    "    for name,attributes in people.items():\n",
    "        pbar.update(1)\n",
    "        if \"summary\" not in attributes:\n",
    "            page=wiki.page(name)\n",
    "            summary=page.summary\n",
    "            #print(name,summary)\n",
    "            people[name][\"summary\"]=summary\n",
    "            i+=1\n",
    "            time.sleep(0.1)\n",
    "        if i==batch_size:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def save_people_json(people,filename):\n",
    "    with open(filename, \"w\") as pfile: json.dump(people,pfile)\n",
    "        \n",
    "def load_people_json(filename):\n",
    "    with open(filename, \"r\") as pfile: \n",
    "        return json.load(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f970da",
   "metadata": {},
   "source": [
    "In the next cell, you will find the code for loading the politician data to a dictionary from the json file that you can download through A+. The commented out code was used to parse and fetch the data. You can inspect how the data was created using that code and the functions in the previous cell. Lateer on in the project, you can use the same code to construct different sets of individuals and fetch data from the Wikipedia by slightly modifying this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6f311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"politicians.json\"\n",
    "#if not os.path.isfile(filename):\n",
    "#    politicians=filter_persons_by(occupation=\"politician\")\n",
    "#    fill_in_genders(politicians)\n",
    "#    save_people_json(politicians,filename)\n",
    "    \n",
    "politicians=load_people_json(filename)\n",
    "\n",
    "## The code below fills in summaries, language editions and links from wikipedia.\n",
    "## The fetching takes place in batches of 1000 queries after which the data is saved to disk.\n",
    "#while fetch_summaries(politicians,batch_size=1000): save_people_json(politicians,filename)\n",
    "#while fetch_langs(politicians,batch_size=1000): save_people_json(politicians,filename)\n",
    "#while fetch_links(politicians,batch_size=1000): save_people_json(politicians,filename)\n",
    "#save_people_json(politicians,filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c784fd8",
   "metadata": {},
   "source": [
    "Below are some lines of code you might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95abd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times the word 'the' appears in all summaries: 36188\n",
      "Natural logarithm of 2.7 is:  0.9932517730102834\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import networkx as nx\n",
    "\n",
    "# Here is some example code for going through politicians and their summaries and counting words\n",
    "allwords={}\n",
    "for name,data in politicians.items():\n",
    "    summary=data[\"summary\"]\n",
    "    for word in summary.split(\" \"): # Splits the summary into words\n",
    "        word=word.strip().strip(\".,\").lower() # Removes white spaces, dots, commas and makes the word lower case\n",
    "        allwords[word]=allwords.get(word,0)+1 # This is a useful pattern for counting numbers of words, the get method returns the value related to word if it is in the dictionary and otherwise 0, so that the counting starts for 0.   \n",
    "\n",
    "# Use the Graph object for constructing your undirected network. See exercise round 5 for examples how to work with networks\n",
    "# You can also consult the documentation of Networkx library online.\n",
    "network = nx.Graph()         \n",
    "        \n",
    "print(\"Number of times the word 'the' appears in all summaries:\",allwords[\"the\"])\n",
    "print(\"Natural logarithm of 2.7 is: \",math.log(2.7)) #taking a logarithm might come in handy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e944ce9",
   "metadata": {},
   "source": [
    "Later on in the project you might want to get more data. The following code gets data for all finnish people who were born between 1900 and 1940. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6699b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching link data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3da0d48e36c4aa9a8edf13170a85c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching summary text data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24988e05f5e640edb11d39855e669423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching language editions data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954add823bd48acb4162fadb796cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename=\"finns-1900-1940.json\"\n",
    "if not os.path.isfile(filename):\n",
    "    people=filter_persons_by(nationality=\"fin\",birth_more=1900,birth_less=1940)\n",
    "    fill_in_genders(people)\n",
    "    save_people_json(people,filename)\n",
    "    \n",
    "people=load_people_json(filename)\n",
    "\n",
    "while fetch_links(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_summaries(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_langs(people,batch_size=1000): save_people_json(people,filename)\n",
    "\n",
    "save_people_json(people,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d10a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching link data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e78af5f916041d99cf3cc5866d8c2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching summary text data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893a7c4698cd42bea8af92e9b2f4e8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching language editions data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b974253aa1432aa543b355f36d4939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename=\"finns-1900-1940.json\"\n",
    "if not os.path.isfile(filename):\n",
    "    people=filter_persons_by(nationality=\"fin\",birth_more=1900,birth_less=1940)\n",
    "    fill_in_genders(people)\n",
    "    save_people_json(people,filename)\n",
    "    \n",
    "people=load_people_json(filename)\n",
    "\n",
    "while fetch_links(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_summaries(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_langs(people,batch_size=1000): save_people_json(people,filename)\n",
    "\n",
    "save_people_json(people,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c37a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching link data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88faa65ae3ed4bb6abf3cf5876bab829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching link data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c31b2c5a6ea4779948f29706134f0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching link data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2ada8a9f904cc999061809194bcae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching summary text data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bedb78b2e547979a54c43dc4852645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching summary text data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc1324373e947ecbf9c28e0d199cebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching summary text data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2b95ed80f6488cbf8bea56f42902ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching language editions data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a3edc14cd14407adf493dd8a76308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching language editions data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7752368463a043b1914874e70506d05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching language editions data from Wikipedia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52174b4354f4df59a539ecdea163d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename=\"artists-1900-current.json\"\n",
    "if not os.path.isfile(filename):\n",
    "   artists=filter_persons_by(occupation=\"artist\", birth_more=1900,birth_less=2023)\n",
    "   fill_in_genders(artists)\n",
    "   save_people_json(artists,filename)\n",
    "\n",
    "people=load_people_json(filename)\n",
    "\n",
    "while fetch_links(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_summaries(people,batch_size=1000): save_people_json(people,filename)\n",
    "while fetch_langs(people,batch_size=1000): save_people_json(people,filename)\n",
    "\n",
    "save_people_json(people,filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
